# CynapticsInduction

**CODE - 1 EXPLANATION :**
I received 95% accuracy for my submission. The code is used to train a machine-learning model that can listen to audio files and identify which class they belong to. First, it sets the folder paths where the training and test audio files are stored. Then it defines a function that turns each audio file into a mel-spectrogram. The code also adds small changes to the audio (like pitch shift and speed change) to make the model learn better. Next, the program goes through all the training folders, reads every .wav file, converts it into a mel-spectrogram, and stores it along with its label. After this, the labels are converted into numbers and the data is split into training and validation sets. A CNN model is then created. It has several layers that help the model learn patterns from the mel-spectrogram images. Dropout and batch normalization are added to prevent overfitting. The model is trained for multiple epochs, while early stopping and learning-rate reduction help improve training stability. Finally, the code loads the test audio files, makes predictions using the trained model, converts those predictions back to their class names, and saves them into a CSV file called submission_cnn_v2.csv.

**CODE - 2 EXPLANATION :**
This code trains a CGAN to generate new audio samples by working with Mel Spectrograms, which are like images of sound. Instead of training on raw audio, which is long and hard for GANs, we convert every sound into a 2D picture using STFT. STFT breaks audio into tiny time windows and finds which frequencies exist in each piece. Then we convert this into a Mel scale so it looks more like how humans hear sound. The dataset class loads each .wav file, turns it into a Mel spectrogram, and makes sure all spectrograms have the same width using MAX_FRAMES. If a spectrogram is too short, it gets padded; if too long, it is cut. This is needed because the GAN trains in batches and all inputs must be the same shape. The Generator takes random noise plus the category label and tries to create a fake Mel spectrogram. It uses ConvTranspose2D layers (upsampling layers) with kernel size, stride, and padding to slowly grow a small feature map into a full spectrogram. ReLU is used so values stay positive, like real Mel spectrograms. The Discriminator takes a spectrogram plus its label and tries to decide if it is real or fake. It uses normal convolution layers with LeakyReLU to avoid dead neurons and allow smoother gradient flow. During training, the Discriminator learns to tell real vs. fake spectrograms, while the Generator learns to fool it. This back-and-forth makes the Generator improve over time. After training, the generated Mel spectrograms are converted back to sound using Griffin-Lim, which rebuilds the missing phase information. Overall, the code trains a GAN that creates completely new audio by generating spectrogram images and turning them back into waveforms.
